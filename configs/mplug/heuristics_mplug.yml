# Network
IMG_RESOLUTION: 512
IMG_FEAT_GRID: 16
IMG_FEAT_SIZE: 4096
BERT_VERSION: bert-large-uncased
MAX_TOKEN: 32
LANG_FEAT_SIZE: 1024
LAYER: 6
HIDDEN_SIZE: 1024
FF_SIZE: 4096
MULTI_HEAD: 8
DROPOUT_R: 0.1
FLAT_MLP_SIZE: 1024
FLAT_GLIMPSES: 1
FLAT_OUT_SIZE: 2048

# Training
BATCH_SIZE: 64
EVAL_BATCH_SIZE: 8
#BERT_LR_MULT: 0.01
#LR_BASE: 0.00005
#LR_DECAY_R: 0.2
#LR_DECAY_LIST: [5,]
#WARMUP_EPOCH: 0
MAX_EPOCH: 10
#GRAD_NORM_CLIP: -1
#OPT: AdamW
#OPT_PARAMS: {betas: '(0.9, 0.98)', eps: '1e-9'}
## optimizer for finetuning warmup (i.e., only update the new appended parameters as a warm-up)
#EPOPH_FTW: 1
#OPT_FTW: Adam
#LR_BASE_FTW: 0.001
#OPT_PARAMS_FTW: {betas: '(0.9, 0.98)', eps: '1e-9'}

#mplug
model: mplug
BertTokenizer: bert-base-uncased
Bert_version_path: 'ckpts/bert-base-uncased'
mplug_optimizer: {opt: adamW, lr1: 3e-5, lr2: 1e-6, weight_decay: 0.02}
mplug_schedular: {sched: cosine, lr: 3e-5, epochs: 15, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-6, warmup_epochs: 4, cooldown_epochs: 0}
deepspeed: True

bert_config: 'configs/config_bert.json'
vision_width: 1024
use_checkpoint: True
clip_path: "ckpts/ViT-L-14"
image_res: 336
text_encoder: 'ckpts/bert-base-uncased'
text_decoder: 'ckpts/bert-base-uncased'
distill: True
beam_size: 20
data_root: datasets
add_ocr: True
add_object: True

warm_up: True
alpha: 0.4

answer_list_path: assets/answer_list.json
k_test: 128
eos: '[SEP]'
distributed: True
Generate_mode: greedy
max_input_length: 80